{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "# the model is organized like this: word = embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../resources/small-embeddings.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def text_embedding(text):\n",
    "    \n",
    "    #it depends if the words have been lowercased or not\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    article_embedd = []\n",
    "    \n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = model[word]\n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\",\"Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\\"\"fixed\\\"\" it for free, and the very next morning I had the same issue. I called to complain, and the \\\"\"manager\\\"\" didn't even apologize!!! So frustrated. Never going back.  They seem overpriced, too.\"\n",
      " \n",
      "\"2\",\"Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.\"\n"
     ]
    }
   ],
   "source": [
    "# YELP product reviews dataset\n",
    "\n",
    "import codecs\n",
    "\n",
    "sentiment_dataset = codecs.open(\"../datasets/yelp-test.csv\",\"r\",\"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "print (sentiment_dataset[1])\n",
    "print (\" \")\n",
    "print (sentiment_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# first, we define two folders, \"corpus\" - with the text and \"labels\", with the labels\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "#for line in sentiment_dataset:\n",
    "for line in sentiment_dataset[:1000]:\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    \n",
    "    emb_text = text_embedding(text)\n",
    "    if len(emb_text) > 0:\n",
    "        corpus.append(emb_text)\n",
    "        labels.append(label)\n",
    "    \n",
    "print (\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we use np array as they are more efficient\n",
    "X = np.array(corpus)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5993431855500821, 0.5969551282051282, 0.595959595959596, None)\n",
      "(0.6496598639455782, 0.6498397435897436, 0.6496847162446202, None)\n",
      "(0.6001838235294117, 0.5873397435897436, 0.5689655172413794, None)\n",
      "(0.5538555691554468, 0.5528846153846154, 0.5488721804511278, None)\n",
      "(0.6939393939393939, 0.6923076923076923, 0.6897207486738064, None)\n",
      "(0.5853858784893268, 0.5833333333333333, 0.5784825371336813, None)\n",
      "(0.6764705882352942, 0.6764705882352942, 0.6764705882352942, None)\n",
      "(0.6185185185185185, 0.6176470588235294, 0.6158088235294118, None)\n",
      "(0.6201264488935722, 0.5931372549019608, 0.564344746162928, None)\n",
      "(0.6273320895522387, 0.6115196078431373, 0.5954950235725511, None)\n",
      " \n",
      "0.6083804477204398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federiconanni/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#here's the documentation: http://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "final_f1 = []\n",
    "# we set that we do 10 fold cross validation\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "# for each of the 10 round\n",
    "for train, test in kf_total:\n",
    "    # we define training and test embeddings and labels\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # we train on the training set, using embeddings and labels\n",
    "    classifier = GaussianNB().fit(X_train , y_train)\n",
    "    \n",
    "    # then we test it on the test set, we provide the embeddings and we make the classifier predict the labels\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # then we compare the prediction with the true test-labels using precision, recall and f1 (ignore the last None column)\n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "    \n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7028985507246377, 0.7019230769230769, 0.6998799519807923, None)\n",
      "(0.6696678671468588, 0.6698717948717949, 0.6697027324592133, None)\n",
      "(0.700487012987013, 0.6979166666666667, 0.6980676328502415, None)\n",
      "(0.6995192307692308, 0.6995192307692308, 0.6995192307692308, None)\n",
      "(0.6193910256410255, 0.6193910256410255, 0.6193910256410255, None)\n",
      "(0.6599025974025974, 0.6578525641025641, 0.6578099838969403, None)\n",
      "(0.6156685808039377, 0.6151960784313726, 0.6151800327332242, None)\n",
      "(0.666530612244898, 0.6666666666666666, 0.6665305705828315, None)\n",
      "(0.6865793780687397, 0.6862745098039216, 0.686356668369954, None)\n",
      "(0.5996677740863787, 0.5980392156862745, 0.5949263502454991, None)\n",
      " \n",
      "0.6607364179528952\n"
     ]
    }
   ],
   "source": [
    "#here's the documentation: http://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(kernel = \"linear\", C=1) \n",
    "\n",
    "final_f1 = []\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    classifier = SVM.fit(X_train , y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6552981552981553, 0.6450320512820513, 0.6419437340153453, None)\n",
      "(0.7131837307152875, 0.6826923076923077, 0.6756982948007113, None)\n",
      "(0.6338481338481339, 0.625, 0.6214833759590792, None)\n",
      "(0.6416666666666666, 0.6362179487179487, 0.6347402597402596, None)\n",
      "(0.6066176470588236, 0.592948717948718, 0.5833333333333334, None)\n",
      "(0.5993431855500821, 0.5969551282051282, 0.595959595959596, None)\n",
      "(0.637218045112782, 0.6341911764705883, 0.6333333333333334, None)\n",
      "(0.5444444444444445, 0.5441176470588236, 0.5437788018433181, None)\n",
      "(0.6066021867115223, 0.6035539215686274, 0.6019997938356871, None)\n",
      "(0.6805084745762712, 0.6740196078431373, 0.6727272727272728, None)\n",
      " \n",
      "0.6204997795547935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "final_f1 = []\n",
    "# we set that we do 10 fold cross validation\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "# for each of the 10 round\n",
    "for train, test in kf_total:\n",
    "    # we define training and test embeddings and labels\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # we train the classifier (using 50 neighbors, but you can change that)\n",
    "    # we train on the training set, using embeddings and labels\n",
    "    classifier = KNeighborsClassifier(n_neighbors=50).fit(X_train, y_train) \n",
    "    \n",
    "    # then we test it on the test set, we provide the embeddings and we make the classifier predict the labels\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # then we compare the prediction with the true test-labels using precision, recall and f1 (ignore the last None column)\n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "    \n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6292653552790044, 0.6290064102564102, 0.6290726817042607, None)\n",
      "(0.5896358543417366, 0.5897435897435898, 0.5896306676008408, None)\n",
      "(0.66, 0.6602564102564102, 0.6598639455782312, None)\n",
      "(0.7011217948717949, 0.7011217948717949, 0.7, None)\n",
      "(0.6696678671468588, 0.6698717948717949, 0.6697027324592133, None)\n",
      "(0.66, 0.6602564102564102, 0.6598639455782312, None)\n",
      "(0.646326530612245, 0.6464460784313726, 0.6463203021333062, None)\n",
      "(0.6562756357670221, 0.6556372549019608, 0.6556873977086743, None)\n",
      "(0.5650572831423895, 0.5649509803921569, 0.5649463464486459, None)\n",
      "(0.615909090909091, 0.6145833333333333, 0.6142329778506973, None)\n",
      " \n",
      "0.6389320997062101\n"
     ]
    }
   ],
   "source": [
    "# homework 2 - nearest centroid\n",
    "\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "final_f1 = []\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    classifier = NearestCentroid().fit(X_train, y_train) \n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
