{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# when should we use a list or a dictionary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35018\n",
      "2236 0.06385287566394426 0.15372980405637676\n"
     ]
    }
   ],
   "source": [
    "# so: how many tweets written by Donald Trump (and only by him!) contain the word \"great\"?\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../datasets/trump.json') as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "print (len(tweets))\n",
    "\n",
    "written_by_trump = 0\n",
    "with_great = 0\n",
    "for tweet in tweets:\n",
    "    if \"Android\" in tweet[\"source\"]:\n",
    "        text = tweet[\"text\"]\n",
    "        written_by_trump += 1\n",
    "        if \"great\" in text.lower():\n",
    "            with_great += 1\n",
    "        \n",
    "print (with_great, with_great/len(tweets),with_great/written_by_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.txt\n",
      "16.txt\n",
      "13.txt\n",
      "12.txt\n",
      "10.txt\n",
      "11.txt\n",
      "9.txt\n",
      "8.txt\n",
      "5.txt\n",
      "4.txt\n",
      "6.txt\n",
      "7.txt\n",
      "3.txt\n",
      "2.txt\n",
      "0.txt\n",
      "1.txt\n",
      "19.txt\n",
      "18.txt\n"
     ]
    }
   ],
   "source": [
    "# we need a few libraries\n",
    "#os is needed to loop over files in a folder\n",
    "# codecs is for encoding a file\n",
    "#BeautifulSoup is needed for parsing the html of a scraped page\n",
    "\n",
    "import codecs,os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we prepare an empty list\n",
    "articles = []\n",
    "\n",
    "# we loop over a folder\n",
    "for filename in os.listdir(\"../datasets/Articles/\"):\n",
    "    # we check if the file is a txt file\n",
    "    if \".txt\" in filename:\n",
    "        print (filename)\n",
    "        # we open and read the file\n",
    "        doc = open(\"../datasets/Articles/\"+filename,\"r\")\n",
    "        doc = doc.read()\n",
    "        \n",
    "        # look at the original HTML - we remove the internet archive toolbar\n",
    "        html_page = str(doc).split(\"<!-- END WAYBACK TOOLBAR INSERT -->\")[1]\n",
    "            \n",
    "        # we parse the page\n",
    "        html_page = BeautifulSoup(html_page, \"lxml\")\n",
    "        \n",
    "        \n",
    "        # we open a new file in writing mode (using codecs) / we need to create the \"CleanedArticles\" folder if it's not there\n",
    "        output = codecs.open(\"../datasets/CleanedArticles/\"+filename,\"w\",\"utf-8\")\n",
    "        \n",
    "        # we define a new list, called text\n",
    "        text = []\n",
    "\n",
    "        # simply take all the paragraphs - we search for the elements \"p\"\n",
    "        for para in html_page.find_all('p'):\n",
    "            # we remove breaklines, tabs etc\n",
    "            para = para.text.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "            text.append(para)\n",
    "        \n",
    "        # we put all paragraphs in a single text\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        # we write the text to the output file\n",
    "        output.write(text)\n",
    "        # we close the output file\n",
    "        output.close()\n",
    "        # we add the text to a list of articles\n",
    "        articles.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertisement By JACEY FORTINDEC. 31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage. (Sydney was among the first major cities to celebrate with fireworks at the stroke of midnight.) In Japan, people paraded in fox masks to attend the first prayer of the year at a Shinto shrine in Tokyo. In the Philippines, revelers gathered — phones in hand — at the Eastwood Mall in Manila to watch balloons and confetti rain down at midnight. Big pots of tea were prepared for New Year’s Eve celebrations in Beijing. The country will also celebrate the Lunar New Year, in February. It was raining in Singapore, but New Year’s Eve celebrants sheltered under umbrellas and raincoats as fireworks sparkled overhead. Tourists donned party hats to watch fireworks in front of the famous Petronas Twin Towers in Kuala Lumpur, Malaysia. Hundreds of couples got married at a mass wedding in Jakarta on New Year’s Eve. We’re interested in your feedback on this page. Tell us what you think. Go to Home Page »\n"
     ]
    }
   ],
   "source": [
    "# let's take the first article of the list of articles\n",
    "article = articles[0]\n",
    "print (article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/federiconanni/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need nltk - one of the most used text processing library in python\n",
    "\n",
    "import nltk # --> documentation: http://www.nltk.org/\n",
    "\n",
    "# you will also need this\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertisement By JACEY FORTINDEC.\n",
      " \n",
      "31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage.\n",
      " \n",
      "(Sydney was among the first major cities to celebrate with fireworks at the stroke of midnight.)\n",
      " \n",
      "In Japan, people paraded in fox masks to attend the first prayer of the year at a Shinto shrine in Tokyo.\n",
      " \n",
      "In the Philippines, revelers gathered — phones in hand — at the Eastwood Mall in Manila to watch balloons and confetti rain down at midnight.\n",
      " \n",
      "Big pots of tea were prepared for New Year’s Eve celebrations in Beijing.\n",
      " \n",
      "The country will also celebrate the Lunar New Year, in February.\n",
      " \n",
      "It was raining in Singapore, but New Year’s Eve celebrants sheltered under umbrellas and raincoats as fireworks sparkled overhead.\n",
      " \n",
      "Tourists donned party hats to watch fireworks in front of the famous Petronas Twin Towers in Kuala Lumpur, Malaysia.\n",
      " \n",
      "Hundreds of couples got married at a mass wedding in Jakarta on New Year’s Eve.\n",
      " \n",
      "We’re interested in your feedback on this page.\n",
      " \n",
      "Tell us what you think.\n",
      " \n",
      "Go to Home Page »\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# we start by dividing the text into sentences\n",
    "sentences = nltk.sent_tokenize(article) # <-- documentation for this command: http://www.nltk.org/_modules/nltk/tokenize.html\n",
    "\n",
    "# let's print all the sentences, so we can exam the quality of the output\n",
    "for sentence in sentences:\n",
    "    print (sentence)\n",
    "    print (\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage.\n"
     ]
    }
   ],
   "source": [
    "# let's consider a single sentence - how do we do that? \n",
    "\n",
    "sentence = sentences[1]\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'In', 'Sydney', ',', 'rainbow', 'fireworks', 'sparkled', 'off', 'the', 'Harbour', 'Bridge', 'in', 'celebration', 'of', 'Australia’s', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "# let's divide the sentence in tokens (aka single words)\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "print (tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31', ',', '2017', 'in', 'sydney', ',', 'rainbow', 'fireworks', 'sparkled', 'off', 'the', 'harbour', 'bridge', 'in', 'celebration', 'of', 'australia’s', 'recent', 'legalization', 'of', 'gay', 'marriage', '.']\n"
     ]
    }
   ],
   "source": [
    "# lower-casing the sentence\n",
    "without_capital_letters = [word.lower() for word in tokenized_sentence]\n",
    "\n",
    "print (without_capital_letters)\n",
    "\n",
    "# homework: write a for-loop for doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# homework: download stopwords <- google it out\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# what is \"stop\" ?\n",
    "\n",
    "print (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "without_stop_words = [word for word in without_capital_letters if word not in stop]\n",
    "\n",
    "print (without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# homework: how do we exclude punctuation? and numbers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
