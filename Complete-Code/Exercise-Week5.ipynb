{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\") if pos[0] == \"V\" else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider two sentences where \"cell\" is mentioned\n",
    "\n",
    "sent1 = \"The terrorist cell was neutralized near the southern Russian city of Makhachkala, the capital of the Republic of Dagestan.\"\n",
    "\n",
    "sent2 = \"The molecule, which uses light energy to move protons across a somatic cell membrane, proved unsuitable for crystallography.\"\n",
    "\n",
    "# you clean the sentences using our pipeline\n",
    "clean_sent1 = nlp_pipeline(sent1)\n",
    "\n",
    "clean_sent2 = nlp_pipeline(sent2)\n",
    "\n",
    "print (\"clean sent 1 (terrorist cell):\", clean_sent1)\n",
    "print (\"clean sent 2 (biological cell):\", clean_sent2)\n",
    "print (\" \")\n",
    "\n",
    "# for each possible sense of \"cell\" you can, for instance, check the overlap between the definition and the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first thing I tried is looking for the overlap of words between the definition and the sentence\n",
    "\n",
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    definition =  sense.definition()\n",
    "    \n",
    "    # you clean the definition with our pipeline\n",
    "    clean_definition = nlp_pipeline(definition)\n",
    "    \n",
    "    # you check the intersection of the two sentences\n",
    "    inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    inters_2 = set(clean_sent2).intersection(clean_definition)\n",
    "    \n",
    "    print (definition)\n",
    "    print (\"clean definition:\", clean_definition)\n",
    "    print (\"intersection with terrorist cell sentence:\", inters_1)\n",
    "    print (\"intersection with biological cell sentence:\", inters_2)\n",
    "    print (len(inters_1),len(inters_2))\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, that doesn't work very well because the are no words in common between the definitions and the sentences. So I tried a different thing, instead of using only the definition I also took into account hypernyms, hyponyms and synonyms to expand the vocabulary of each sense\n",
    "\n",
    "This way -- at least -- you can capture a couple of words, but you can see how hard it is when you look simply for words in common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    definition = sense.definition()    \n",
    "    \n",
    "    # take all hypernyms, hyponyms and synonyms - you need to do a bit of cleaning\n",
    "    hypernyms = [hyper.lemmas()[0].name().replace(\"_\",\" \") for hyper in sense.hypernyms()] \n",
    "    \n",
    "    hyponyms = [hypon.lemmas()[0].name().replace(\"_\",\" \") for hypon in sense.hyponyms()] \n",
    "        \n",
    "    synonyms = [synon.replace(\"_\",\" \") for synon in sense.lemma_names()] \n",
    "    \n",
    "    # you concatenate all of them - check out online how to use \"join\" to connect elements of a list\n",
    "    sense_text = sense.definition() + \" \".join(sense.examples())  + \" \".join(hypernyms)+ \" \".join(hyponyms)+ \" \".join(synonyms)\n",
    "\n",
    "    # now you have a very long definition, which uses all these pieces of information\n",
    "    clean_definition = nlp_pipeline(sense_text)\n",
    "    \n",
    "\n",
    "    inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    inters_2 = set(clean_sent2).intersection(clean_definition)    \n",
    "    \n",
    "    print (definition)\n",
    "    print (\"clean definition:\", clean_definition)\n",
    "    print (\"intersection with terrorist cell sentence:\", inters_1)\n",
    "    print (\"intersection with biological cell sentence:\", inters_2)\n",
    "    print (len(inters_1),len(inters_2))\n",
    "    print (\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
